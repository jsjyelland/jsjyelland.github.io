# Visualising a dataset using t-SNE

1. TOC
{:toc}

## Dealing with lots of dimensions

When doing deep learning, we often want to plot the data we're working with to understand the relationships. Unfortunately, we can't visualise anything above 3 dimensions and even on plots 3 dimensions can be difficult to accurately convey.

Because of this, there are techniques for projecting data in high dimensions down into low dimensional space so we can view the relationships. One of these is called t-stochastic neighbor embedding, or t-SNE.

## What's t-SNE?

According to the [scikit learn implementaiton](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), t-SNE is a tool for visualising high-dimensional data, by converting similarities between data points to joint probablilites, and then tries to minimise the Kullback-Leibler divergence (a type of statistical distance) of these probabilities between the high dimensional and low dimensional data.

## Visualising the MNIST data set

To show how t-SNE works, we'll use the MNIST data set, which contains a large collection of hand-drawn digits in 28x28 black and white format. An example looks like this:

![](/images/0.png "A zero from the MNIST data set")

When it comes to trying to visualise the way that digts compare to each other, we're dealing with 28x28=784 dimensions ðŸ¤¯.

Given that we can't even visualise 4D easily, there's no way to see how the different images are related in 784-d space. So we'll use t-SNE to try and represent the images in 2 dimensions instead.

Let's load the set in to a fastai python program.

```python
from fastbook import *
from sklearn.manifold import TSNE

# Download the data set
path = untar_data(URLs.MNIST)
Path.BASE_PATH = path

print((path/'training').ls())
```
    (#10) [Path('training/0'),Path('training/1'),Path('training/2'),Path('training/3'),Path('training/4'),Path('training/5'),Path('training/6'),Path('training/7'),Path('training/8'),Path('training/9')]

There's a folder with each of the digits 0-9 in it.

Now we can load the images in as tensors.

```python
# Load in the images

# Store an array of labels and images (these arrays will be the same size)
labels = []
images = []

for i in range(0, 10):
    images.extend(tensor(Image.open(o)) for o in (path/'training'/f'{i}').ls().sorted())
    labels.extend(i for o in (path/'training'/f'{i}').ls().sorted())
```

Once these tensors are loaded into an array, we can use `torch.stack()` to combine into one long tensor.

```python
# Stack into one tensor and convert to values between 0 and 1
X = torch.stack(images).float() / 255

print(X.shape)
```

    torch.Size([60000, 28, 28])

So we now have 60000 28x28 images. We can use sklearn's `TSNE` class to easily perform t-SNE. We just have to reshape our `X` tensor so that it's of dimensions 600000x784, meaning that each image has been flattened into a vector.

```python
# Create the TSNE class to transform the data
tsne = TSNE(n_components=2, random_state=14)
# We need to reshape the data so that the images are flattened into one dimension of length 784.
X_tsne = tsne.fit_transform(X.reshape(60000, 28*28))
```

Once we have the lower dimensional data, we can plot using matplotlib.

```python
# Plot the result

plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c = labels, cmap = plt.cm.get_cmap('tab10', 10), s = 40)
plt.title('t-SNE of MNIST data set')
plt.xlabel('Axis 1')
plt.ylabel('Axis 2')
plt.colorbar(ticks = range(10));
```

And this gives the following result:

![](/images/tsne.png "A plot of the digits (coloured coded from 0-9) in the reduced 2-dimensional space")

So we can see how t-SNE can represent the similarities in high dimensional space. Note how, except for some outliers, most coloured dots are in areas according to their colour.

## Conclusion

In conclusion, t-SNE is a very powerful tool for mapping really high dimensional space down to lower, visualisable dimensional space. In this case, we were able to map 784 dimensions down to 2 dimensions and view the relationships between data.

This technique can also be applied to viewing the activations of different layers in a neural network, which are also very high dimensional spaces.