# Fast.ai makes getting started with deep learning almost too easy

1. TOC
{:toc}

## My deep learning background

Before starting the computer vision course, I had previously taken the machine learning course at UQ. Even though deep learning was covered in this course, the content was more of an overview of lots of machine learning techniques, of which deep learning was just one. We didn't dive very deep into any particular topic or cover specfics of implementation. This left me with the idea still in my head that deep learning was some sort of black magic that involved needing to become a wizard at using TensorFlow or the like, and took ages to master. Thankfully, fast.ai came along and changed my perception entirely.

## Introducing Fast.ai

The [fast.ai book](https://course.fast.ai/Resources/book.html) makes it incredibly easy to grasp the concepts invovled in deep learning by starting at the high level and developing complete deep learning models, before diving in to the details and explaning the building blocks.

Even by the second or third chapter, I realised just how simple the [fast.ai python library](https://docs.fast.ai/) made constructing a deep learning model. They have built extremely intuitive and convenient wrappers around PyTorch so that you don't have to write as much boilerplate code every time you want to start on a project and get a proof of concept up and running.

## Let's build a simple classifier

To show how easy it is, I'll run through an example of building an image classifier to distinguish between different types of vehicles.

### Creating the data set

Before we can train a model, we need a data set to train from. Luckily, fast.ai includes a very easy to use inbuilt image scraper, which downloads images from Duck Duck go.

```python
urls = search_images_ddg('cat', max_images=5)
```

We firstly need to set up a list of vehicle types we'd like to recognise. We'll train our model to distinguish between cars, busses, trucks and motorbikes.

```python
from fastbook import *

# Setup the list of vehicles to search for
vehicle_types = 'car', 'bus', 'truck', 'motorbike'
```

We can then iterate over our list of vehicle types and scrape 100 from Duck Duck go for each. We'll be using python's really handy `Path` class to create the path to store the images in.

To make labelling the data easy, we're going to put each image in a subfolder named with the type. This way we don't have to worry about the name of the file.

```python
# Image destination
image_path = Path('vehicles')

# Make sure the folder exists
if not image_path.exists():
    image_path.mkdir()

    # Download the images
    for vehicle in vehicle_types:
        dest = (image_path/vehicle)
        urls = search_images_ddg(vehicle, max_images=100)
        download_images(dest, urls=urls)

    images = get_image_files(image_path)

    # Delete any corrupt images
    verify_images(images).map(Path.unlink)
```

The images look like this:
```python
print(images[0])
```

    vehicles\bus\04db6934-23aa-4513-af32-bcbf91e88c05.jpg

Here's an example of what we downloaded:
![](/images/car.jpg "An example of a car scraped from Duck Duck Go")

Now we can load these images into a data set. Fast.ai makes this really easy with its `DataBlock` class, which lets us specify the format for our data set, including how to label the data and split into training and validation.

We're going to label the images according to their parent folder, and fast.ai has a convient function to do this, `parent_label()`. The `splitter` parameter tells the `DataBlock` how we'd like to create the training/validation set split. In this case, we're asking for 20% of the data to be put into the validation set. The seed means that we'll get the same 20% every time we run the code. 

The model we'll use as a starting point expects images of size 224x224 pixels, so we use a `RandomResizedCrop` to crop the iamge to size when training. It's random so that we don't always lose the same amount of an image, and can train the model to include all the bits of an image in different steps of the training. The `min_scale` says don't scale in to more than 50% of the original area.

```python
# Set up our data block
vehicle_data_block = DataBlock(
    blocks=(ImageBlock, CategoryBlock), # Data is images, label is a category
    get_items=get_image_files, # We'll load in our data by looking for images in a directory
    splitter=RandomSplitter(valid_pct=0.2, seed=42), # 20% of the images are for validation
    get_y=parent_label, # Use the parent folder to label the data
    item_tfms=RandomResizedCrop(224, min_scale=0.5), # Random crop to the right size for training
    batch_tfms=aug_transforms() # Data augmentation
)

# Load our images using this data format
vehicle_data = vehicle_data_block.dataloaders(image_path)
```

To make the model robust, we're using a technique called data augmentation which makes a series of modifications to the images during training, such as rotating, stretching or lighting adjustments. This is to make the model more robust to variations in these things such as you might expect when showing the model different photos of the same thing. We don't want it to learn only to recognise a car when looking at it straight on, for example.

```python
# Inspect some variations of the training data
vehicle_data.train.show_batch(max_n=8, nrows=2, unique=True)
```

![](/images/data_aug.png "Data augmentation on a truck, showing a sample of the training data with different angles, stretches and reflections of the same image.")

### Building the model

Constructing the model is extremely simple thanks to a fast.ai function called `vision_learner()`. We can pass in a pre-trained model and the dataset we want to use, and it will modify the model so that the output has the correct number of activations for the classification we want to do. In this case, we want our model to have 4 output activiations corresponding to the four classes.

Using a pre-trained model allows us to take advantage of work that's already been done, instead of having to reinvent the wheel. The ResNet model has been designed and trained on a dataset known as ImageNet, which contains many different types of images and classification labels already - so it's well suited to this task. We're using the `resnet18` architecture, which is a convolutional nerual network with 18 layers.

```python
# Build the model using the pretrained ResNet architecture
learn = vision_learner(vehicle_data, resnet18, metrics=error_rate)
```

### Training the model

It's really that simple! All that's left is to train the model. fast.ai makes that incredibly simple too - just 1 line of code.

```python
# Fine-tune for 10 epochs
learn.fine_tune(10)
```

On my machine, with only a NVidia GTX 1070-Ti graphics card, that only took 2 and a half minutes to train. Not very long!

### Results

We can see how the model improved over time, by plotting the loss:

```python
learn.recorder.plot_loss()
```

![](/images/loss.png "The training & validation loss of the model")

By constructing a `ClassificationInterpretation`, we can see some more info about how our model is performing.

```python
interp = ClassificationInterpretation.from_learner(learn)
interp.print_classification_report()
```

| | precision | recall | f1-score | support |
|-|-|-|-|-|
| bus | 0.91 | 1.00 | 0.95 | 21 |
|          car | 0.95 | 0.86 | 0.90 | 21 |
|    motorbike | 1.00 | 1.00 | 1.00 | 14 |
|        truck | 0.88 | 0.88 | 0.88 | 17 |
|     accuracy |      |      | 0.93 | 73 |
|    macro avg | 0.94 | 0.93 | 0.93 | 73 |
| weighted avg | 0.93 | 0.93 | 0.93 | 73 |

With only that little bit of effort, we've got a model that's 93% accurate. Not perfect, but pretty good!

```python
# Plot the confusion matrix
interp.plot_confusion_matrix()
```

We can have a look at the confusion matrix. This shows us a matrix of how many images were predicted to be in each category, versus which category they actually belonged to.

![](/images/confusion.png "The confusion matrix")

This model is pretty good! We're seeing mostly big numbers on the diagonal, which is what we want. There's a few mistakes though, for example there was 1 truck that the model thought was a bus, and one that it thought was a car.

We can visualise these losses using the following code:

```python
# 10 top losses
interp.plot_top_losses(10)
```

![](/images/top_loss.png "The 10 images with the most loss in the validation set")

We can see what happened with the 5 mistakes, as well as the other images that the model wasn't so condfident on. One of these shows an example of bad labelling. The image on the middle left, which is supposedly a truck, looks more like a car. So in this case, the model was right!

## Conclusion

In summary, it doesn't take a lot of work to achieve pretty good results when using the fast.ai library and taking advantage of pre-trained models. With a bit more work, this could be made to have even higher accuracy than shown here.